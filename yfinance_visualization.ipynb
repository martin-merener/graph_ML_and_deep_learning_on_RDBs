{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00cb0533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install yfinance pandas numpy matplotlib lxml requests\n",
    "import datetime as dt\n",
    "import io\n",
    "import math\n",
    "import re\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1) Get current S&P 500 tickers from Wikipedia ---------------------------\n",
    "def get_sp500_tickers() -> List[str]:\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "    # pandas.read_html can parse the table directly\n",
    "    tables = pd.read_html(url, flavor=\"lxml\")\n",
    "    # The first table is the constituents table on Wikipedia\n",
    "    df = tables[0]\n",
    "    tickers = df[\"Symbol\"].astype(str).tolist()\n",
    "    # Yahoo uses '-' instead of '.' (e.g., BRK.B -> BRK-B, BF.B -> BF-B)\n",
    "    tickers = [t.replace(\".\", \"-\").strip() for t in tickers]\n",
    "    # Deduplicate and keep simple sanity filters\n",
    "    tickers = sorted(set([t for t in tickers if re.fullmatch(r\"[A-Z0-9\\-]+\", t)]))\n",
    "    return tickers\n",
    "\n",
    "# --- 2) Download daily data (Open, High) for ~1y -----------------------------\n",
    "def fetch_ohlc(tickers: List[str], start: str, end: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with MultiIndex columns (Field, Ticker) when group_by='column'.\n",
    "    We'll request in batches to be gentle and reduce timeouts.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    batch_size = 60  # yfinance handles threading internally\n",
    "    for i in range(0, len(tickers), batch_size):\n",
    "        batch = tickers[i:i+batch_size]\n",
    "        df = yf.download(\n",
    "            tickers=batch,\n",
    "            start=start,\n",
    "            end=end,\n",
    "            interval=\"1d\",\n",
    "            auto_adjust=False,\n",
    "            group_by=\"column\",\n",
    "            threads=True,\n",
    "            progress=False,\n",
    "        )\n",
    "        # Ensure 2-level columns: (Field, Ticker)\n",
    "        if isinstance(df.columns, pd.MultiIndex):\n",
    "            all_data.append(df.loc[:, [\"Open\", \"High\"]])\n",
    "        else:\n",
    "            # Single-ticker edge case: add ticker level\n",
    "            df2 = pd.concat(\n",
    "                {batch[0]: df[[\"Open\", \"High\"]]},\n",
    "                axis=1\n",
    "            ).swaplevel(axis=1)\n",
    "            all_data.append(df2)\n",
    "    if not all_data:\n",
    "        raise RuntimeError(\"No data downloaded. Check network or tickers.\")\n",
    "    data = pd.concat(all_data, axis=1).sort_index(axis=1)\n",
    "    # Reorder columns to (Field, Ticker)\n",
    "    data = data.reindex([\"Open\", \"High\"], axis=1, level=0)\n",
    "    return data\n",
    "\n",
    "# --- 3) Compute daily change rate: (High / Open) - 1 -------------------------\n",
    "def compute_change_rates(ohl: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    ohl: MultiIndex columns (Field, Ticker), index = DatetimeIndex\n",
    "    Returns a single Series of all (High/Open - 1) values stacked across tickers & days.\n",
    "    \"\"\"\n",
    "    open_df = ohl[\"Open\"]\n",
    "    high_df = ohl[\"High\"]\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        rates = (high_df / open_df) - 1.0\n",
    "    # Drop invalid/zero-open rows\n",
    "    rates = rates.replace([np.inf, -np.inf], np.nan).dropna(how=\"all\")\n",
    "    # Flatten to long vector\n",
    "    long = rates.stack(dropna=True)  # index: (date, ticker)\n",
    "    # By definition High >= Open; negative values could occur only if data glitches exist—clip them.\n",
    "    long = long.clip(lower=0)\n",
    "    return long.rename(\"change_rate\")\n",
    "\n",
    "# --- 4) Bin into [0%,1%), [1%,2%), ... with overflow -------------------------\n",
    "def bin_and_count(values: pd.Series, max_pct_cap: int | None = None) -> pd.DataFrame:\n",
    "    max_observed_pct = int(math.ceil(values.max() * 100.0))\n",
    "    # cap the upper bound to keep the histogram readable\n",
    "    if max_pct_cap is None:\n",
    "        # Most days will be < 10% from open to high; still, be safe up to 30%\n",
    "        max_pct_cap = max(10, min(30, max_observed_pct))\n",
    "    edges = np.arange(0, max_pct_cap + 1) / 100.0  # e.g., 0.00, 0.01, ..., 0.30\n",
    "    # Create an overflow bin for >= last edge\n",
    "    counts, _ = np.histogram(values, bins=np.append(edges, np.inf))\n",
    "    labels = [f\"[{i}%,{i+1}%)\" for i in range(0, max_pct_cap)] + [f\">={max_pct_cap}%\"]\n",
    "    out = pd.DataFrame({\"bin\": labels, \"count\": counts})\n",
    "    out[\"share\"] = out[\"count\"] / counts.sum()\n",
    "    return out\n",
    "\n",
    "# --- 5) Main -----------------------------------------------------------------\n",
    "def main():\n",
    "    # Choose a recent 365-day window (end yesterday to avoid partial current day)\n",
    "    tz = dt.timezone(dt.timedelta(hours=-4))  # Toronto (EDT) summer offset; not critical here\n",
    "    end_date = dt.datetime.now(tz).date() - dt.timedelta(days=1)\n",
    "    start_date = end_date - dt.timedelta(days=365)\n",
    "\n",
    "    print(\"Fetching S&P 500 tickers from Wikipedia...\")\n",
    "    tickers = get_sp500_tickers()\n",
    "    print(f\"Tickers found: {len(tickers)}\")\n",
    "\n",
    "    print(f\"Downloading daily data from {start_date} to {end_date} ...\")\n",
    "    ohl = fetch_ohlc(tickers, start=start_date.isoformat(), end=end_date.isoformat())\n",
    "\n",
    "    print(\"Computing (High / Open) - 1 ...\")\n",
    "    all_rates = compute_change_rates(ohl)\n",
    "    n_days = len(pd.bdate_range(start_date, end_date))  # trading days proxy\n",
    "    print(f\"Data points (stock-days) collected: {len(all_rates):,} \"\n",
    "          f\"(expected up to ~{len(tickers)*n_days:,})\")\n",
    "\n",
    "    print(\"Binning into 1% buckets from 0% upward (with overflow)...\")\n",
    "    table = bin_and_count(all_rates, max_pct_cap=None)\n",
    "    print(\"\\nHistogram counts:\")\n",
    "    print(table.to_string(index=False, formatters={\"share\": \"{:.2%}\".format}))\n",
    "\n",
    "    # --- Plot (simple matplotlib histogram of the raw values) -----------------\n",
    "    # Use explicit bin edges matching the table, plus overflow\n",
    "    max_bin = int(math.ceil(all_rates.max() * 100.0))\n",
    "    max_bin = max(10, min(30, max_bin))  # keep readable; change if you want\n",
    "    edges = np.append(np.arange(0, max_bin + 1) / 100.0, np.inf)\n",
    "\n",
    "    plt.figure(figsize=(9, 5.5))\n",
    "    plt.hist(all_rates.values, bins=edges)\n",
    "    plt.title(\"S&P 500 (current constituents): Distribution of Daily (High / Open − 1)\\nLast ~365 days\")\n",
    "    plt.xlabel(\"Intraday upside from Open to High (fraction)\")\n",
    "    plt.ylabel(\"Count of stock-days\")\n",
    "    # Tick labels as percents up to the cap\n",
    "    xticks = np.arange(0, max_bin + 1) / 100.0\n",
    "    plt.xticks(xticks, [f\"{int(x*100)}%\" for x in xticks], rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     try:\n",
    "#         main()\n",
    "#     except Exception as e:\n",
    "#         print(\"ERROR:\", e, file=sys.stderr)\n",
    "#         print(\"\\nTips:\\n\"\n",
    "#               \"• Make sure you have internet access for Wikipedia and Yahoo Finance.\\n\"\n",
    "#               \"• If Wikipedia blocks read_html, replace get_sp500_tickers() with a static CSV of tickers.\\n\"\n",
    "#               \"• If some tickers fail, lower the batch_size or re-run; yfinance can be flaky.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95ff3785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements:\n",
    "#   pip install pandas yfinance requests lxml\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "\n",
    "WIKI_SP500_URL = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "\n",
    "def load_sp500_constituents() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns DataFrame with columns: symbol, company_name, yahoo_symbol\n",
    "    - 'symbol' is canonical S&P 500 ticker (e.g., 'BRK.B')\n",
    "    - 'yahoo_symbol' is Yahoo Finance format (e.g., 'BRK-B')\n",
    "    \"\"\"\n",
    "    tbl = pd.read_html(WIKI_SP500_URL, flavor=\"lxml\")[0]\n",
    "    tbl = tbl.rename(columns={\"Symbol\": \"symbol\", \"Security\": \"company_name\"})\n",
    "    tbl = tbl[[\"symbol\", \"company_name\"]].copy()\n",
    "    tbl[\"yahoo_symbol\"] = tbl[\"symbol\"].str.replace(\".\", \"-\", regex=False)\n",
    "    return tbl\n",
    "\n",
    "def download_sp500_ohlc(spx_df: pd.DataFrame, period_days: int = 220, show_progress: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download daily OHLC (we keep Open/High) for all S&P 500 tickers over ~period_days.\n",
    "    Returns a LONG DataFrame with columns:\n",
    "        date (datetime64[ns]), symbol, company_name, Open, High\n",
    "\n",
    "    Tip: choose period_days >= lookback_days + buffer (e.g., +60).\n",
    "    \"\"\"\n",
    "    tickers = spx_df[\"yahoo_symbol\"].tolist()\n",
    "\n",
    "    data = yf.download(\n",
    "        tickers=tickers,\n",
    "        period=f\"{period_days}d\",\n",
    "        interval=\"1d\",\n",
    "        group_by=\"ticker\",\n",
    "        auto_adjust=False,\n",
    "        threads=True,\n",
    "        progress=show_progress,\n",
    "    )\n",
    "\n",
    "    records = []\n",
    "    # Mapping back to canonical symbols and names\n",
    "    name_map = dict(zip(spx_df[\"yahoo_symbol\"], spx_df[\"company_name\"]))\n",
    "    sym_map  = dict(zip(spx_df[\"yahoo_symbol\"], spx_df[\"symbol\"]))\n",
    "\n",
    "    if isinstance(data.columns, pd.MultiIndex):\n",
    "        # (ticker, field)\n",
    "        for ysym in tickers:\n",
    "            cols_needed = [(ysym, \"Open\"), (ysym, \"High\")]\n",
    "            if all(c in data.columns for c in cols_needed):\n",
    "                sub = data.loc[:, cols_needed].dropna()\n",
    "                if sub.empty:\n",
    "                    continue\n",
    "                sub.columns = [\"Open\", \"High\"]\n",
    "                sub = sub[(sub[\"Open\"] > 0) & sub[\"High\"].notna()]\n",
    "                for dt, row in sub.iterrows():\n",
    "                    records.append({\n",
    "                        \"date\": pd.to_datetime(dt),\n",
    "                        \"symbol\": sym_map.get(ysym, ysym),\n",
    "                        \"company_name\": name_map.get(ysym, \"\"),\n",
    "                        \"Open\": float(row[\"Open\"]),\n",
    "                        \"High\": float(row[\"High\"]),\n",
    "                    })\n",
    "    else:\n",
    "        # Single-ticker fallback (unlikely for the whole index)\n",
    "        if set([\"Open\", \"High\"]).issubset(set(data.columns)):\n",
    "            ysym = tickers[0]\n",
    "            sub = data[[\"Open\", \"High\"]].dropna()\n",
    "            sub = sub[(sub[\"Open\"] > 0) & sub[\"High\"].notna()]\n",
    "            for dt, row in sub.iterrows():\n",
    "                records.append({\n",
    "                    \"date\": pd.to_datetime(dt),\n",
    "                    \"symbol\": sym_map.get(ysym, ysym),\n",
    "                    \"company_name\": name_map.get(ysym, \"\"),\n",
    "                    \"Open\": float(row[\"Open\"]),\n",
    "                    \"High\": float(row[\"High\"]),\n",
    "                })\n",
    "\n",
    "    prices = pd.DataFrame.from_records(records, columns=[\"date\", \"symbol\", \"company_name\", \"Open\", \"High\"])\n",
    "    # Ensure types and ordering\n",
    "    prices.sort_values([\"date\", \"symbol\"], inplace=True, kind=\"mergesort\")\n",
    "    prices.reset_index(drop=True, inplace=True)\n",
    "    return prices\n",
    "\n",
    "def _last_trading_days_from_prices(prices_df: pd.DataFrame, lookback_days: int) -> pd.DatetimeIndex:\n",
    "    \"\"\"\n",
    "    Get the last N unique trading dates present in the prices DataFrame.\n",
    "    \"\"\"\n",
    "    uds = pd.DatetimeIndex(sorted(prices_df[\"date\"].dt.normalize().unique()))\n",
    "    return uds[-lookback_days:] if len(uds) >= lookback_days else uds\n",
    "\n",
    "def find_open_to_high_spikes(\n",
    "    prices_df: pd.DataFrame,\n",
    "    pct_threshold: float = 0.05,   # 5%\n",
    "    lookback_days: int = 100\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pure-analysis step (no downloads). Assumes prices_df has columns:\n",
    "        date, symbol, company_name, Open, High\n",
    "\n",
    "    Returns DataFrame:\n",
    "        date, symbol, company_name, pct_increase  (pct_increase in %)\n",
    "    \"\"\"\n",
    "    assert {\"date\", \"symbol\", \"company_name\", \"Open\", \"High\"}.issubset(prices_df.columns), \\\n",
    "        \"prices_df missing required columns.\"\n",
    "\n",
    "    # Limit to last N trading days *present in the df*\n",
    "    last_days = _last_trading_days_from_prices(prices_df, lookback_days)\n",
    "    sub = prices_df[prices_df[\"date\"].dt.normalize().isin(last_days)].copy()\n",
    "    if sub.empty:\n",
    "        return pd.DataFrame(columns=[\"date\", \"symbol\", \"company_name\", \"pct_increase\"])\n",
    "\n",
    "    sub = sub[(sub[\"Open\"] > 0) & sub[\"High\"].notna()].copy()\n",
    "    sub[\"pct_increase\"] = (sub[\"High\"] - sub[\"Open\"]) / sub[\"Open\"]\n",
    "\n",
    "    hits = sub[sub[\"pct_increase\"] >= pct_threshold].copy()\n",
    "    if hits.empty:\n",
    "        return pd.DataFrame(columns=[\"date\", \"symbol\", \"company_name\", \"pct_increase\"])\n",
    "\n",
    "    hits[\"pct_increase\"] = (hits[\"pct_increase\"] * 100.0).round(4)\n",
    "    out = hits.loc[:, [\"date\", \"symbol\", \"company_name\", \"pct_increase\"]].copy()\n",
    "    out.sort_values(by=[\"date\", \"pct_increase\"], ascending=[True, False], inplace=True, kind=\"mergesort\")\n",
    "    out.reset_index(drop=True, inplace=True)\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage\n",
    "# -----------------------------\n",
    "# if __name__ == \"__main__\":\n",
    "#     # 1) Load S&P 500 constituents once\n",
    "#     spx = load_sp500_constituents()\n",
    "\n",
    "#     # 2) Download price history ONCE (choose enough days for your largest intended lookback)\n",
    "#     #    e.g., if you'll try lookback_days up to 150, use period_days >= 150 + 60\n",
    "#     prices = download_sp500_ohlc(spx, period_days=240, show_progress=False)\n",
    "\n",
    "#     # (Optional) Save to CSV to reuse later without re-downloading:\n",
    "#     # prices.to_csv(\"sp500_prices_open_high.csv\", index=False)\n",
    "\n",
    "#     # (Optional) Later, you can load from CSV instead of downloading:\n",
    "#     # prices = pd.read_csv(\"sp500_prices_open_high.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "#     # 3) Run analysis many times WITHOUT re-downloading:\n",
    "#     df_5pct_100d = find_open_to_high_spikes(prices, pct_threshold=0.05, lookback_days=100)\n",
    "#     print(df_5pct_100d.head(20))\n",
    "\n",
    "#     # Try different thresholds/lookbacks with the SAME in-memory DataFrame:\n",
    "#     # df_7pct_80d = find_open_to_high_spikes(prices, pct_threshold=0.07, lookback_days=80)\n",
    "#     # df_3pct_150d = find_open_to_high_spikes(prices, pct_threshold=0.03, lookback_days=150)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1ec4535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements (pick what you use):\n",
    "#   pip install requests feedparser python-dateutil pytz\n",
    "#   # Optional if using Finnhub:\n",
    "#   # pip install requests\n",
    "#\n",
    "# Usage preview (after your spikes df):\n",
    "#   news_df = fetch_prev_day_news_for_hits(\n",
    "#       hits_df=df_5pct_100d,           # columns: date, symbol, company_name, pct_increase\n",
    "#       provider=\"auto\",                 # \"auto\" | \"finnhub\" | \"googlenews\"\n",
    "#       tz_str=\"America/New_York\"\n",
    "#   )\n",
    "#   print(news_df.head())\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import feedparser\n",
    "from dateutil import tz\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"news_fetch\")\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "FIN_KEYWORDS = [\n",
    "    # earnings & outlook\n",
    "    \"earnings\", \"results\", \"q1\", \"q2\", \"q3\", \"q4\", \"quarter\", \"guidance\", \"forecast\",\n",
    "    \"revenue\", \"sales\", \"profit\", \"eps\", \"net income\", \"operating income\", \"margin\",\n",
    "    # corporate actions / capital\n",
    "    \"dividend\", \"buyback\", \"repurchase\", \"stock split\", \"secondary offering\", \"ipo\",\n",
    "    \"spinoff\", \"spin-off\", \"spac\",\n",
    "    # m&a & partnerships\n",
    "    \"acquisition\", \"acquire\", \"merger\", \"merge\", \"takeover\", \"deal\", \"partnership\",\n",
    "    # ratings & coverage\n",
    "    \"upgrade\", \"downgrade\", \"price target\", \"initiates coverage\", \"maintains\",\n",
    "    # operations / strategy\n",
    "    \"layoff\", \"restructuring\", \"guidance\", \"product launch\", \"recall\", \"contract\",\n",
    "    \"order\", \"production\", \"factory\", \"plant\", \"strike\", \"union\",\n",
    "    # regulatory & legal\n",
    "    \"sec\", \"complaint\", \"investigation\", \"probe\", \"lawsuit\", \"settlement\",\n",
    "    \"approval\", \"antitrust\", \"regulator\", \"fda\", \"fcc\",\n",
    "    # risk / distress\n",
    "    \"bankruptcy\", \"chapter 11\", \"default\", \"liquidity\", \"restatement\", \"data breach\",\n",
    "]\n",
    "\n",
    "RE_WORD = re.compile(r\"[A-Za-z0-9#\\-\\$]+\")\n",
    "\n",
    "def _clean_text(s: str) -> str:\n",
    "    return \" \".join(RE_WORD.findall((s or \"\").lower()))\n",
    "\n",
    "def prev_calendar_day(d: pd.Timestamp, tz_str: str = \"America/New_York\") -> Tuple[datetime, datetime]:\n",
    "    \"\"\"\n",
    "    For a trading date d (naive or tz-aware), return previous calendar day's [start,end] in the given timezone.\n",
    "    \"\"\"\n",
    "    if isinstance(d, pd.Timestamp):\n",
    "        dt_utc = d.tz_localize(\"UTC\") if d.tzinfo is None else d.tz_convert(\"UTC\")\n",
    "        local = dt_utc.tz_convert(tz_str)\n",
    "        local_prev = (local - timedelta(days=1)).date()\n",
    "    else:\n",
    "        local_prev = d.date() - timedelta(days=1)\n",
    "    tzinfo = tz.gettz(tz_str)\n",
    "    start = datetime(local_prev.year, local_prev.month, local_prev.day, 0, 0, 0, tzinfo=tzinfo)\n",
    "    end   = datetime(local_prev.year, local_prev.month, local_prev.day, 23, 59, 59, tzinfo=tzinfo)\n",
    "    # Normalize to naive UTC for providers that want UTC or ISO8601 with Z\n",
    "    return (start.astimezone(tz.UTC).replace(tzinfo=None),\n",
    "            end.astimezone(tz.UTC).replace(tzinfo=None))\n",
    "\n",
    "def strip_legal_suffixes(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove common legal suffixes to broaden matches (Inc, Corp, plc, Ltd, Company, Class A, etc.)\n",
    "    \"\"\"\n",
    "    if not name:\n",
    "        return name\n",
    "    s = re.sub(r\",?\\s+(inc\\.?|corp\\.?|corporation|co\\.?|company|plc|ltd\\.?|nv|sa|ag|class\\s+[a-z])\\b\\.?\", \"\", name, flags=re.I)\n",
    "    return \" \".join(s.split())\n",
    "\n",
    "def build_company_terms(symbol: str, company_name: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Return a set of query terms (ticker + cleaned company name + well-known alias if any).\n",
    "    \"\"\"\n",
    "    base = strip_legal_suffixes(company_name)\n",
    "    terms = {symbol.upper(), base}\n",
    "    # minimal alias map for common rebrands\n",
    "    ALIASES = {\n",
    "        \"META PLATFORMS\": [\"Facebook\", \"FB\", \"Meta\"],\n",
    "        \"ALPHABET\": [\"Google\", \"GOOGL\"],\n",
    "        \"PARAMOUNT GLOBAL\": [\"ViacomCBS\"],\n",
    "        \"KENVUE\": [\"Johnson & Johnson consumer health\"],\n",
    "        \"VERIZON COMMUNICATIONS\": [\"Verizon\"],\n",
    "        \"AT&T\": [\"ATT\", \"AT&T Inc\"],\n",
    "        \"BLOCK\": [\"Square\"],\n",
    "    }\n",
    "    key = base.upper()\n",
    "    if key in ALIASES:\n",
    "        terms.update(ALIASES[key])\n",
    "    return list(terms)\n",
    "\n",
    "def financial_relevance_score(title: str, summary: str = \"\") -> int:\n",
    "    \"\"\"\n",
    "    Simple keyword score. Increase threshold if you want stricter filtering.\n",
    "    \"\"\"\n",
    "    text = _clean_text(f\"{title} {summary}\")\n",
    "    score = sum(1 for kw in FIN_KEYWORDS if kw in text)\n",
    "    return score\n",
    "\n",
    "# -----------------------------\n",
    "# Provider: Finnhub (preferred if available)\n",
    "# -----------------------------\n",
    "def fetch_finnhub_news(symbol: str, start_dt_utc: datetime, end_dt_utc: datetime, token: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Finnhub company-news endpoint (UTC date range).\n",
    "    Docs: https://finnhub.io/docs/api/company-news\n",
    "    \"\"\"\n",
    "    # Finnhub expects YYYY-MM-DD (UTC)\n",
    "    frm = start_dt_utc.date().isoformat()\n",
    "    to  = end_dt_utc.date().isoformat()\n",
    "    url = \"https://finnhub.io/api/v1/company-news\"\n",
    "    params = {\"symbol\": symbol.upper(), \"from\": frm, \"to\": to, \"token\": token}\n",
    "    r = requests.get(url, params=params, timeout=20)\n",
    "    if r.status_code != 200:\n",
    "        logger.warning(\"Finnhub error %s for %s %s..%s\", r.status_code, symbol, frm, to)\n",
    "        return []\n",
    "    items = r.json() or []\n",
    "    # Normalize\n",
    "    out = []\n",
    "    for it in items:\n",
    "        # it fields: category, datetime (unix), headline, id, image, related, source, summary, url\n",
    "        pub_ts = datetime.utcfromtimestamp(it.get(\"datetime\", 0))\n",
    "        out.append({\n",
    "            \"title\": it.get(\"headline\", \"\"),\n",
    "            \"summary\": it.get(\"summary\", \"\"),\n",
    "            \"url\": it.get(\"url\", \"\"),\n",
    "            \"source\": it.get(\"source\", \"\"),\n",
    "            \"published_utc\": pub_ts,\n",
    "            \"provider\": \"finnhub\",\n",
    "        })\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "# Provider: Google News RSS (fallback, no key)\n",
    "# -----------------------------\n",
    "def fetch_google_news(company_terms: List[str], start_dt_utc: datetime, end_dt_utc: datetime) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Query Google News RSS for each company term, then filter by UTC day window.\n",
    "    No key needed, best-effort. Respect rate limits.\n",
    "    \"\"\"\n",
    "    BASE = \"https://news.google.com/rss/search\"\n",
    "    results = {}\n",
    "    for term in company_terms:\n",
    "        # Construct query: exact phrase OR ticker; fetch recent (\"when:7d\"), filter in code\n",
    "        q = f'\"{term}\" when:7d'\n",
    "        params = {\"q\": q, \"hl\": \"en-US\", \"gl\": \"US\", \"ceid\": \"US:en\"}\n",
    "        resp = requests.get(BASE, params=params, timeout=20)\n",
    "        time.sleep(0.3)  # be polite\n",
    "        feed = feedparser.parse(resp.text)\n",
    "        for entry in feed.entries:\n",
    "            # published_parsed is time.struct_time in local tz? Treat as UTC-like, then parse from 'published'\n",
    "            pub = None\n",
    "            if hasattr(entry, \"published_parsed\") and entry.published_parsed:\n",
    "                pub = datetime(*entry.published_parsed[:6])\n",
    "            elif hasattr(entry, \"updated_parsed\") and entry.updated_parsed:\n",
    "                pub = datetime(*entry.updated_parsed[:6])\n",
    "            title = entry.title if hasattr(entry, \"title\") else \"\"\n",
    "            summary = entry.summary if hasattr(entry, \"summary\") else \"\"\n",
    "            link = entry.link if hasattr(entry, \"link\") else \"\"\n",
    "            source = entry.source.title if hasattr(entry, \"source\") and hasattr(entry.source, \"title\") else \"GoogleNews\"\n",
    "\n",
    "            if pub is None:\n",
    "                continue\n",
    "\n",
    "            # Keep only those within [start_dt_utc, end_dt_utc] (approximate; RSS timezones may vary)\n",
    "            if not (start_dt_utc <= pub <= end_dt_utc):\n",
    "                continue\n",
    "\n",
    "            # Deduplicate by URL\n",
    "            key = hashlib.md5(link.encode(\"utf-8\")).hexdigest()\n",
    "            if key not in results:\n",
    "                results[key] = {\n",
    "                    \"title\": title,\n",
    "                    \"summary\": summary,\n",
    "                    \"url\": link,\n",
    "                    \"source\": source,\n",
    "                    \"published_utc\": pub,\n",
    "                    \"provider\": \"googlenews\",\n",
    "                }\n",
    "    return list(results.values())\n",
    "\n",
    "# -----------------------------\n",
    "# Coordinator with caching\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class NewsConfig:\n",
    "    provider: str = \"auto\"          # \"auto\" | \"finnhub\" | \"googlenews\"\n",
    "    tz_str: str = \"America/New_York\"\n",
    "    min_relevance: int = 1          # increase to 2 for stricter filtering\n",
    "    require_company_mention: bool = True  # require company name (or alias) in title/summary\n",
    "    sleep_between_calls: float = 0.0      # you can increase for rate limiting\n",
    "\n",
    "def fetch_prev_day_news_for_hits(\n",
    "    hits_df: pd.DataFrame,\n",
    "    provider: str = \"auto\",\n",
    "    tz_str: str = \"America/New_York\",\n",
    "    min_relevance: int = 1,\n",
    "    require_company_mention: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each row in hits_df (date, symbol, company_name), fetch previous calendar day's news articles\n",
    "    that are financially relevant. Returns a dataframe:\n",
    "\n",
    "    columns: trading_date, symbol, company_name, pct_increase,\n",
    "             news_published_utc, source, title, url, relevance_score, provider\n",
    "    \"\"\"\n",
    "    assert {\"date\", \"symbol\", \"company_name\"}.issubset(hits_df.columns), \\\n",
    "        \"hits_df must have columns: date, symbol, company_name\"\n",
    "\n",
    "    cfg = NewsConfig(provider=provider, tz_str=tz_str, min_relevance=min_relevance,\n",
    "                     require_company_mention=require_company_mention)\n",
    "\n",
    "    # Choose provider automatically\n",
    "    finnhub_token = os.environ.get(\"FINNHUB_TOKEN\", \"\").strip()\n",
    "    effective_provider = cfg.provider\n",
    "    if effective_provider == \"auto\":\n",
    "        effective_provider = \"finnhub\" if finnhub_token else \"googlenews\"\n",
    "    logger.info(\"Using news provider: %s\", effective_provider)\n",
    "\n",
    "    # Cache to avoid re-calling same (symbol, prev_day) during this run\n",
    "    cache: Dict[Tuple[str, str], List[dict]] = {}\n",
    "\n",
    "    rows = []\n",
    "    # Grouping avoids repeated calls when same symbol appears with multiple spike dates\n",
    "    for (sym, comp), grp in hits_df.groupby([\"symbol\", \"company_name\"]):\n",
    "        terms = build_company_terms(sym, comp)\n",
    "        for trading_date, sub in grp.groupby(\"date\"):\n",
    "            trading_ts = pd.Timestamp(trading_date)\n",
    "            start_utc, end_utc = prev_calendar_day(trading_ts, tz_str=cfg.tz_str)\n",
    "            cache_key = (sym.upper(), start_utc.date().isoformat())\n",
    "\n",
    "            if cache_key in cache:\n",
    "                articles = cache[cache_key]\n",
    "            else:\n",
    "                if effective_provider == \"finnhub\" and finnhub_token:\n",
    "                    articles = fetch_finnhub_news(sym, start_utc, end_utc, finnhub_token)\n",
    "                else:\n",
    "                    articles = fetch_google_news(terms, start_utc, end_utc)\n",
    "                cache[cache_key] = articles\n",
    "                if cfg.sleep_between_calls > 0:\n",
    "                    time.sleep(cfg.sleep_between_calls)\n",
    "\n",
    "            # filter for financial relevance\n",
    "            for a in articles:\n",
    "                title = a.get(\"title\", \"\")\n",
    "                summary = a.get(\"summary\", \"\")\n",
    "                url = a.get(\"url\", \"\")\n",
    "                source = a.get(\"source\", \"\")\n",
    "                pub = a.get(\"published_utc\", None)\n",
    "                provider_name = a.get(\"provider\", effective_provider)\n",
    "\n",
    "                if not title or not url or pub is None:\n",
    "                    continue\n",
    "\n",
    "                score = financial_relevance_score(title, summary)\n",
    "                if score < cfg.min_relevance:\n",
    "                    continue\n",
    "\n",
    "                if cfg.require_company_mention:\n",
    "                    text = _clean_text(f\"{title} {summary}\")\n",
    "                    # require at least one company term appears (loosely)\n",
    "                    if not any(_clean_text(t) in text for t in terms if _clean_text(t)):\n",
    "                        continue\n",
    "\n",
    "                # Add a row per spike (symbol,date) so schema matches one-to-many\n",
    "                for _, spike_row in sub.iterrows():\n",
    "                    rows.append({\n",
    "                        \"trading_date\": pd.to_datetime(spike_row[\"date\"]).date(),\n",
    "                        \"symbol\": sym,\n",
    "                        \"company_name\": comp,\n",
    "                        \"pct_increase\": float(spike_row.get(\"pct_increase\", float(\"nan\"))),\n",
    "                        \"news_published_utc\": pd.to_datetime(pub),\n",
    "                        \"source\": source,\n",
    "                        \"title\": title,\n",
    "                        \"url\": url,\n",
    "                        \"relevance_score\": int(score),\n",
    "                        \"provider\": provider_name,\n",
    "                    })\n",
    "\n",
    "    news_df = pd.DataFrame(rows, columns=[\n",
    "        \"trading_date\", \"symbol\", \"company_name\", \"pct_increase\",\n",
    "        \"news_published_utc\", \"source\", \"title\", \"url\", \"relevance_score\", \"provider\"\n",
    "    ])\n",
    "    news_df.sort_values([\"trading_date\", \"symbol\", \"news_published_utc\"], inplace=True)\n",
    "    news_df.reset_index(drop=True, inplace=True)\n",
    "    return news_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08a34087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load S&P 500 constituents once\n",
    "spx = load_sp500_constituents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee202651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Download price history ONCE (choose enough days for your largest intended lookback)\n",
    "#    e.g., if you'll try lookback_days up to 150, use period_days >= 150 + 60\n",
    "prices = download_sp500_ohlc(spx, period_days=240, show_progress=False)\n",
    "\n",
    "\n",
    "# (Optional) Save to CSV to reuse later without re-downloading:\n",
    "# prices.to_csv(\"sp500_prices_open_high.csv\", index=False)\n",
    "\n",
    "# (Optional) Later, you can load from CSV instead of downloading:\n",
    "# prices = pd.read_csv(\"sp500_prices_open_high.csv\", parse_dates=[\"date\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ce3edf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) Run analysis many times WITHOUT re-downloading:\n",
    "#df_5pct_100d = find_open_to_high_spikes(prices, pct_threshold=0.05, lookback_days=100)\n",
    "#print(df_5pct_100d.head(20))\n",
    "\n",
    "# Try different thresholds/lookbacks with the SAME in-memory DataFrame:\n",
    "df_7pct_30d = find_open_to_high_spikes(prices, pct_threshold=0.07, lookback_days=30)\n",
    "# df_3pct_150d = find_open_to_high_spikes(prices, pct_threshold=0.03, lookback_days=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97a1ef68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39, 4), 21)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_7pct_30d.shape, df_7pct_30d['date'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad77089e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>symbol</th>\n",
       "      <th>company_name</th>\n",
       "      <th>pct_increase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-07-08</td>\n",
       "      <td>MRNA</td>\n",
       "      <td>Moderna</td>\n",
       "      <td>11.8684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-07-08</td>\n",
       "      <td>ALB</td>\n",
       "      <td>Albemarle Corporation</td>\n",
       "      <td>9.2988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-07-08</td>\n",
       "      <td>FCX</td>\n",
       "      <td>Freeport-McMoRan</td>\n",
       "      <td>7.5777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-07-08</td>\n",
       "      <td>INTC</td>\n",
       "      <td>Intel</td>\n",
       "      <td>7.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-07-08</td>\n",
       "      <td>APA</td>\n",
       "      <td>APA Corporation</td>\n",
       "      <td>7.0175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-07-09</td>\n",
       "      <td>PTC</td>\n",
       "      <td>PTC Inc.</td>\n",
       "      <td>18.8933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-07-09</td>\n",
       "      <td>ENPH</td>\n",
       "      <td>Enphase Energy</td>\n",
       "      <td>8.9218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-07-10</td>\n",
       "      <td>UAL</td>\n",
       "      <td>United Airlines Holdings</td>\n",
       "      <td>7.3171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-07-18</td>\n",
       "      <td>IVZ</td>\n",
       "      <td>Invesco</td>\n",
       "      <td>12.0112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-07-22</td>\n",
       "      <td>IQV</td>\n",
       "      <td>IQVIA</td>\n",
       "      <td>10.9730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-07-22</td>\n",
       "      <td>GPC</td>\n",
       "      <td>Genuine Parts Company</td>\n",
       "      <td>9.0969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-07-22</td>\n",
       "      <td>IVZ</td>\n",
       "      <td>Invesco</td>\n",
       "      <td>8.2335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2025-07-22</td>\n",
       "      <td>ENPH</td>\n",
       "      <td>Enphase Energy</td>\n",
       "      <td>7.6674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2025-07-23</td>\n",
       "      <td>BKR</td>\n",
       "      <td>Baker Hughes</td>\n",
       "      <td>8.1687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2025-07-23</td>\n",
       "      <td>TEL</td>\n",
       "      <td>TE Connectivity</td>\n",
       "      <td>8.1022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2025-07-25</td>\n",
       "      <td>MOH</td>\n",
       "      <td>Molina Healthcare</td>\n",
       "      <td>7.3637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2025-07-28</td>\n",
       "      <td>SMCI</td>\n",
       "      <td>Supermicro</td>\n",
       "      <td>7.6510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2025-07-29</td>\n",
       "      <td>INCY</td>\n",
       "      <td>Incyte</td>\n",
       "      <td>8.1528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2025-07-30</td>\n",
       "      <td>GNRC</td>\n",
       "      <td>Generac</td>\n",
       "      <td>10.2424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2025-07-31</td>\n",
       "      <td>GNRC</td>\n",
       "      <td>Generac</td>\n",
       "      <td>8.9392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2025-08-01</td>\n",
       "      <td>MPWR</td>\n",
       "      <td>Monolithic Power Systems</td>\n",
       "      <td>8.5135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2025-08-01</td>\n",
       "      <td>ALGN</td>\n",
       "      <td>Align Technology</td>\n",
       "      <td>7.7519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2025-08-04</td>\n",
       "      <td>PCG</td>\n",
       "      <td>PG&amp;E Corporation</td>\n",
       "      <td>7.1378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2025-08-05</td>\n",
       "      <td>AXON</td>\n",
       "      <td>Axon Enterprise</td>\n",
       "      <td>8.3032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2025-08-05</td>\n",
       "      <td>BR</td>\n",
       "      <td>Broadridge Financial Solutions</td>\n",
       "      <td>7.2698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2025-08-06</td>\n",
       "      <td>AIZ</td>\n",
       "      <td>Assurant</td>\n",
       "      <td>8.2477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2025-08-06</td>\n",
       "      <td>MTCH</td>\n",
       "      <td>Match Group</td>\n",
       "      <td>7.2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2025-08-07</td>\n",
       "      <td>VST</td>\n",
       "      <td>Vistra Corp.</td>\n",
       "      <td>11.3243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2025-08-07</td>\n",
       "      <td>PSKY</td>\n",
       "      <td>Paramount Skydance Corporation</td>\n",
       "      <td>9.8797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2025-08-11</td>\n",
       "      <td>TKO</td>\n",
       "      <td>TKO Group Holdings</td>\n",
       "      <td>10.5292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2025-08-12</td>\n",
       "      <td>PSKY</td>\n",
       "      <td>Paramount Skydance Corporation</td>\n",
       "      <td>10.4594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2025-08-12</td>\n",
       "      <td>UAL</td>\n",
       "      <td>United Airlines Holdings</td>\n",
       "      <td>7.4573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2025-08-12</td>\n",
       "      <td>MCHP</td>\n",
       "      <td>Microchip Technology</td>\n",
       "      <td>7.3620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2025-08-12</td>\n",
       "      <td>DAL</td>\n",
       "      <td>Delta Air Lines</td>\n",
       "      <td>7.0358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2025-08-13</td>\n",
       "      <td>PSKY</td>\n",
       "      <td>Paramount Skydance Corporation</td>\n",
       "      <td>56.5179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2025-08-13</td>\n",
       "      <td>WBD</td>\n",
       "      <td>Warner Bros. Discovery</td>\n",
       "      <td>8.0389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2025-08-14</td>\n",
       "      <td>INTC</td>\n",
       "      <td>Intel</td>\n",
       "      <td>10.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2025-08-15</td>\n",
       "      <td>ENPH</td>\n",
       "      <td>Enphase Energy</td>\n",
       "      <td>15.0875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2025-08-15</td>\n",
       "      <td>FSLR</td>\n",
       "      <td>First Solar</td>\n",
       "      <td>13.5165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date symbol                    company_name  pct_increase\n",
       "0  2025-07-08   MRNA                         Moderna       11.8684\n",
       "1  2025-07-08    ALB           Albemarle Corporation        9.2988\n",
       "2  2025-07-08    FCX                Freeport-McMoRan        7.5777\n",
       "3  2025-07-08   INTC                           Intel        7.2588\n",
       "4  2025-07-08    APA                 APA Corporation        7.0175\n",
       "5  2025-07-09    PTC                        PTC Inc.       18.8933\n",
       "6  2025-07-09   ENPH                  Enphase Energy        8.9218\n",
       "7  2025-07-10    UAL        United Airlines Holdings        7.3171\n",
       "8  2025-07-18    IVZ                         Invesco       12.0112\n",
       "9  2025-07-22    IQV                           IQVIA       10.9730\n",
       "10 2025-07-22    GPC           Genuine Parts Company        9.0969\n",
       "11 2025-07-22    IVZ                         Invesco        8.2335\n",
       "12 2025-07-22   ENPH                  Enphase Energy        7.6674\n",
       "13 2025-07-23    BKR                    Baker Hughes        8.1687\n",
       "14 2025-07-23    TEL                 TE Connectivity        8.1022\n",
       "15 2025-07-25    MOH               Molina Healthcare        7.3637\n",
       "16 2025-07-28   SMCI                      Supermicro        7.6510\n",
       "17 2025-07-29   INCY                          Incyte        8.1528\n",
       "18 2025-07-30   GNRC                         Generac       10.2424\n",
       "19 2025-07-31   GNRC                         Generac        8.9392\n",
       "20 2025-08-01   MPWR        Monolithic Power Systems        8.5135\n",
       "21 2025-08-01   ALGN                Align Technology        7.7519\n",
       "22 2025-08-04    PCG                PG&E Corporation        7.1378\n",
       "23 2025-08-05   AXON                 Axon Enterprise        8.3032\n",
       "24 2025-08-05     BR  Broadridge Financial Solutions        7.2698\n",
       "25 2025-08-06    AIZ                        Assurant        8.2477\n",
       "26 2025-08-06   MTCH                     Match Group        7.2004\n",
       "27 2025-08-07    VST                    Vistra Corp.       11.3243\n",
       "28 2025-08-07   PSKY  Paramount Skydance Corporation        9.8797\n",
       "29 2025-08-11    TKO              TKO Group Holdings       10.5292\n",
       "30 2025-08-12   PSKY  Paramount Skydance Corporation       10.4594\n",
       "31 2025-08-12    UAL        United Airlines Holdings        7.4573\n",
       "32 2025-08-12   MCHP            Microchip Technology        7.3620\n",
       "33 2025-08-12    DAL                 Delta Air Lines        7.0358\n",
       "34 2025-08-13   PSKY  Paramount Skydance Corporation       56.5179\n",
       "35 2025-08-13    WBD          Warner Bros. Discovery        8.0389\n",
       "36 2025-08-14   INTC                           Intel       10.0500\n",
       "37 2025-08-15   ENPH                  Enphase Energy       15.0875\n",
       "38 2025-08-15   FSLR                     First Solar       13.5165"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_7pct_30d.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d49126b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:news_fetch:Using news provider: googlenews\n"
     ]
    }
   ],
   "source": [
    "# After you build your spikes dataframe (e.g., df_5pct_100d):\n",
    "news_df = fetch_prev_day_news_for_hits(\n",
    "    hits_df=df_5pct_100d,\n",
    "    provider=\"auto\",                  # \"auto\" tries Finnhub, else Google News\n",
    "    tz_str=\"America/New_York\",\n",
    "    min_relevance=1,                  # raise to 2 for stricter filtering\n",
    "    require_company_mention=True\n",
    ")\n",
    "\n",
    "# Optionally save\n",
    "# news_df.to_csv(\"spike_prevday_news.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ed53368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trading_date</th>\n",
       "      <th>symbol</th>\n",
       "      <th>company_name</th>\n",
       "      <th>pct_increase</th>\n",
       "      <th>news_published_utc</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>provider</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>2025-08-15</td>\n",
       "      <td>ENPH</td>\n",
       "      <td>Enphase Energy</td>\n",
       "      <td>15.0875</td>\n",
       "      <td>2025-08-13 09:05:29</td>\n",
       "      <td>MarketBeat</td>\n",
       "      <td>Enphase Energy, Inc. (NASDAQ:ENPH) Shares Sold...</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMi3AFBV...</td>\n",
       "      <td>2</td>\n",
       "      <td>googlenews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>2025-08-15</td>\n",
       "      <td>ENPH</td>\n",
       "      <td>Enphase Energy</td>\n",
       "      <td>15.0875</td>\n",
       "      <td>2025-08-13 13:30:47</td>\n",
       "      <td>Energy-Storage.News</td>\n",
       "      <td>'We will be good for the distant future': Enph...</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMi0gFBV...</td>\n",
       "      <td>1</td>\n",
       "      <td>googlenews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>2025-08-15</td>\n",
       "      <td>ENPH</td>\n",
       "      <td>Enphase Energy</td>\n",
       "      <td>15.0875</td>\n",
       "      <td>2025-08-13 17:07:00</td>\n",
       "      <td>TradingView</td>\n",
       "      <td>Enphase Energy Strengthens Battery Storage Foo...</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMiuAFBV...</td>\n",
       "      <td>2</td>\n",
       "      <td>googlenews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>2025-08-15</td>\n",
       "      <td>ENPH</td>\n",
       "      <td>Enphase Energy</td>\n",
       "      <td>15.0875</td>\n",
       "      <td>2025-08-13 18:18:07</td>\n",
       "      <td>Barchart.com</td>\n",
       "      <td>Enphase Energy Strengthens Battery Storage Foo...</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMiqgFBV...</td>\n",
       "      <td>1</td>\n",
       "      <td>googlenews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>2025-08-15</td>\n",
       "      <td>ENPH</td>\n",
       "      <td>Enphase Energy</td>\n",
       "      <td>15.0875</td>\n",
       "      <td>2025-08-13 18:33:32</td>\n",
       "      <td>AInvest</td>\n",
       "      <td>Based on the current analyst ratings, is ENPH ...</td>\n",
       "      <td>https://news.google.com/rss/articles/CBMipgFBV...</td>\n",
       "      <td>1</td>\n",
       "      <td>googlenews</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    trading_date symbol    company_name  pct_increase  news_published_utc  \\\n",
       "191   2025-08-15   ENPH  Enphase Energy       15.0875 2025-08-13 09:05:29   \n",
       "192   2025-08-15   ENPH  Enphase Energy       15.0875 2025-08-13 13:30:47   \n",
       "193   2025-08-15   ENPH  Enphase Energy       15.0875 2025-08-13 17:07:00   \n",
       "194   2025-08-15   ENPH  Enphase Energy       15.0875 2025-08-13 18:18:07   \n",
       "195   2025-08-15   ENPH  Enphase Energy       15.0875 2025-08-13 18:33:32   \n",
       "\n",
       "                  source                                              title  \\\n",
       "191           MarketBeat  Enphase Energy, Inc. (NASDAQ:ENPH) Shares Sold...   \n",
       "192  Energy-Storage.News  'We will be good for the distant future': Enph...   \n",
       "193          TradingView  Enphase Energy Strengthens Battery Storage Foo...   \n",
       "194         Barchart.com  Enphase Energy Strengthens Battery Storage Foo...   \n",
       "195              AInvest  Based on the current analyst ratings, is ENPH ...   \n",
       "\n",
       "                                                   url  relevance_score  \\\n",
       "191  https://news.google.com/rss/articles/CBMi3AFBV...                2   \n",
       "192  https://news.google.com/rss/articles/CBMi0gFBV...                1   \n",
       "193  https://news.google.com/rss/articles/CBMiuAFBV...                2   \n",
       "194  https://news.google.com/rss/articles/CBMiqgFBV...                1   \n",
       "195  https://news.google.com/rss/articles/CBMipgFBV...                1   \n",
       "\n",
       "       provider  \n",
       "191  googlenews  \n",
       "192  googlenews  \n",
       "193  googlenews  \n",
       "194  googlenews  \n",
       "195  googlenews  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.loc[news_df.symbol == \"ENPH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2317a3ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df[['trading_date', 'symbol']].drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "add396f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1375, 4)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_5pct_100d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401a8cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
