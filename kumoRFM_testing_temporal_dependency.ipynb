{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a1e2cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kumoai.experimental.rfm as rfm, os\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60032560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-08-08 17:07:09 - kumoai:196 - INFO] Successfully initialized the Kumo SDK against deployment https://kumorfm.ai/api, with log level INFO.\n"
     ]
    }
   ],
   "source": [
    "home_api_key_file = Path.home() / \"kumoai_key.txt\"\n",
    "with open(home_api_key_file, \"r\") as file:\n",
    "    api_key = file.read().strip()\n",
    "os.environ[\"KUMO_API_KEY\"] = api_key\n",
    "\n",
    "rfm.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b872da07",
   "metadata": {},
   "source": [
    "## Creation of synthetic data: 4 tables\n",
    "\n",
    "Create a synthetic dataset with the purpose of testing how KumoRFM deals with temporal dependency that is not in the transactional history, but instead in a user type variable that can change over time.\n",
    "Essentially, there are users of 2 types (premium/free), and premium users can do actions that free users cannot.\n",
    "The exercise is about testing if KumoRFM detects this dependency and include them into its predictions.\n",
    "\n",
    "### Dataset description\n",
    "We create a synthetic dataset representing users of 2 types (tier free or tier premium), uploading files, but the tier can change over time.\n",
    "While on Premium tier, users can upload any size. And while on free tier, users can upload files below 10GB. \n",
    "For simplicity, there are only 2 sizes: 5GB and 50GB.\n",
    "Users can change their tier no earlier than 24 hours after the last change. \n",
    "Within an hour of becoming premium, the likelihood of the user uploading a 50GB is higher.\n",
    "There are 100 users, covering 5 cohorts:\n",
    "- Always premium.\n",
    "- Always free.\n",
    "- Premium --> Free (once)\n",
    "- Free --> Premium (once)\n",
    "- Free --> Premium --> Free (twice, each change >= 24 hours apart)\n",
    "The history of transactions last 10 days from March 1st to March 10th, 2025.\n",
    "The prediction tasks will be done for different users, at different points in time, predicting their likelihood of uploading a 50GB file within the next hour.\n",
    "The expectations are: is that for users that just became Free, this should be 0. For users that just became Premium it should be high. For users that had been Premium for a while it should be >> 0. \n",
    "\n",
    "### Tables\n",
    "\n",
    "- users\n",
    "    - user_id (PK)\n",
    "    - name\n",
    "\n",
    "- tiers (user tiers: free/premium, temporal, non-overalapping for each user)\n",
    "    - user_id (FK -> users.user_id)\n",
    "    - from_datetime\n",
    "    - until_datetime (NULL means \"still in effect\")\n",
    "    - tier in {'free', 'premium'}\n",
    "    - Composite PK: user_id, from_datetime)\n",
    "\n",
    "- items\n",
    "    - item_id (PK)\n",
    "    - size_gb in {5, 50}\n",
    "\n",
    "- uploads\n",
    "    - txn_id (PK)\n",
    "    - user_id (FK -> users.user_id)\n",
    "    - item_id (KF -> items.item_id)\n",
    "    - datetime\n",
    "\n",
    "### Cohorts (20 users each)\n",
    "\n",
    "- Always premium (unknown start) → a single interval that spans the whole window.\n",
    "- Always free → a single interval that spans the whole window.\n",
    "- Premium → Free once (≥24h after start).\n",
    "- Free → Premium once (≥24h).\n",
    "- Free → Premium → Free (each change ≥24h apart; all within window).\n",
    "\n",
    "\n",
    "### Behavior rules:\n",
    "\n",
    "- While free: upload only 5 GB items.\n",
    "    - Per-user free-upload rate: draw from, say, Poisson(λ_free_user) per day with λ sampled from a user-level distribution (to inject heterogeneity).\n",
    "- While premium: upload both 5 GB and 50 GB.\n",
    "    - Immediately after becoming premium: within the first hour, probability of at least one 50 GB upload is very high (e.g., 85–95%).\n",
    "    - After that first hour, keep premium uploads going with a higher base rate than free; 50 GB vs 5 GB split could be, e.g., 40–60%, tapering (optional) as time from upgrade increases.\n",
    "- Hard rule: probability of 50 GB exactly at the moment of switching to free is 0 (and remains 0 while free).\n",
    "- We’ll jitter timestamps to look realistic (uniform within hours/minutes), and we’ll keep all uploads inside the user’s current tier interval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69966c58",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "- items is only 2 rows. We do not distinguish between items: just make it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "807bf04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: users.csv, tiers.csv, items.csv, uploads.csv\n",
      "Integrity check passed: No 50GB uploads while free.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generate a small relational dataset (CSV) for graph + prediction testing.\n",
    "\n",
    "Tables:\n",
    "- users(user_id PK, name)\n",
    "- tiers(user_id FK, from_datetime, until_datetime, tier)  # composite PK (user_id, from_datetime)\n",
    "- items(item_id PK, size_gb)                              # exactly two rows: (1,5), (2,50)\n",
    "- uploads(txn_id PK, user_id FK, item_id FK, datetime)\n",
    "\n",
    "Cohorts (20 users each):\n",
    "1) Always premium\n",
    "2) Always free\n",
    "3) Premium -> Free (once)\n",
    "4) Free -> Premium (once)\n",
    "5) Free -> Premium -> Free (twice; each change >= 24h apart)\n",
    "\n",
    "Behavior:\n",
    "- While free: uploads only 5GB (item_id=1).\n",
    "- While premium: uploads both 5GB and 50GB (item_id ∈ {1,2}).\n",
    "- Within 1 hour after becoming premium: very high chance (90%) of at least one 50GB upload.\n",
    "- Exactly when/while free: 50GB uploads are impossible.\n",
    "- Upload rates are user-specific and higher on premium than free.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import csv\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------\n",
    "# Configuration\n",
    "# ------------------------\n",
    "SEED = 42\n",
    "N_USERS = 100\n",
    "START = datetime(2025, 3, 1, 0, 0, 0, tzinfo=timezone.utc)\n",
    "END = datetime(2025, 3, 11, 0, 0, 0, tzinfo=timezone.utc)  # end-exclusive (covers Mar 1–10)\n",
    "MIN_GAP_BETWEEN_TIER_CHANGES = timedelta(hours=24)\n",
    "\n",
    "# Upload rate modeling (per-hour) via user-specific lognormal draws\n",
    "FREE_RATE_DAY_MEAN = 1.0   # typical free uploads per day\n",
    "FREE_RATE_DAY_STD = 0.7\n",
    "PREM_RATE_DAY_MEAN = 3.0   # typical premium uploads per day\n",
    "PREM_RATE_DAY_STD = 1.0\n",
    "\n",
    "# Size selection while premium (outside the \"first hour\" spike)\n",
    "BASE_P50_WHILE_PREMIUM = 0.4  # 40% chance of 50GB for non-spike premium uploads\n",
    "\n",
    "# Spike rules when becoming premium\n",
    "P_SPIKE_50GB_WITHIN_1H = 0.90\n",
    "MAX_SPIKE_50GB = 2  # cap the number of spike 50GB uploads within the first hour\n",
    "\n",
    "# Output files\n",
    "USERS_CSV = \"users.csv\"\n",
    "TIERS_CSV = \"tiers.csv\"\n",
    "ITEMS_CSV = \"items.csv\"\n",
    "UPLOADS_CSV = \"uploads.csv\"\n",
    "\n",
    "# Fixed items table: exactly two rows\n",
    "ITEMS = [\n",
    "    {\"item_id\": 1, \"size_gb\": 5},\n",
    "    {\"item_id\": 2, \"size_gb\": 50},\n",
    "]\n",
    "ITEM_SIZE_BY_ID = {1: 5, 2: 50}\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "@dataclass\n",
    "class TierInterval:\n",
    "    user_id: int\n",
    "    from_dt: datetime\n",
    "    until_dt: datetime  # end-exclusive\n",
    "    tier: str           # 'free' | 'premium'\n",
    "\n",
    "def iso(dt: datetime) -> str:\n",
    "    return dt.astimezone(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "def draw_user_rate(mean_per_day: float, std_per_day: float) -> float:\n",
    "    \"\"\"Draw a user-specific rate (per hour) using a lognormal around mean/std per day.\"\"\"\n",
    "    mean = max(0.01, mean_per_day)\n",
    "    std = max(0.01, std_per_day)\n",
    "    cv2 = (std / mean) ** 2\n",
    "    sigma2 = math.log(cv2 + 1.0)\n",
    "    sigma = math.sqrt(sigma2)\n",
    "    mu = math.log(mean) - 0.5 * sigma2\n",
    "    per_day = np.random.lognormal(mean=mu, sigma=sigma)\n",
    "    return float(per_day / 24.0)\n",
    "\n",
    "def bounded_random_time(start: datetime, end: datetime) -> datetime:\n",
    "    \"\"\"Uniform random time in [start, end).\"\"\"\n",
    "    if end <= start:\n",
    "        return start\n",
    "    span = (end - start).total_seconds()\n",
    "    offs = random.random() * span\n",
    "    return start + timedelta(seconds=offs)\n",
    "\n",
    "def generate_cohort_tiers(user_id: int, cohort: int) -> list[TierInterval]:\n",
    "    \"\"\"Create tier intervals for a single user according to cohort.\"\"\"\n",
    "    t0 = START\n",
    "    tE = END\n",
    "\n",
    "    def random_change_time(low: datetime, high: datetime) -> datetime:\n",
    "        lo = low + MIN_GAP_BETWEEN_TIER_CHANGES\n",
    "        hi = high - MIN_GAP_BETWEEN_TIER_CHANGES\n",
    "        if hi <= lo:\n",
    "            return low + (high - low) / 2\n",
    "        return bounded_random_time(lo, hi)\n",
    "\n",
    "    intervals: list[TierInterval] = []\n",
    "\n",
    "    if cohort == 1:  # Always premium\n",
    "        intervals.append(TierInterval(user_id, t0, tE, \"premium\"))\n",
    "\n",
    "    elif cohort == 2:  # Always free\n",
    "        intervals.append(TierInterval(user_id, t0, tE, \"free\"))\n",
    "\n",
    "    elif cohort == 3:  # Premium -> Free (once)\n",
    "        c1 = random_change_time(t0, tE)\n",
    "        intervals.append(TierInterval(user_id, t0, c1, \"premium\"))\n",
    "        intervals.append(TierInterval(user_id, c1, tE, \"free\"))\n",
    "\n",
    "    elif cohort == 4:  # Free -> Premium (once)\n",
    "        c1 = random_change_time(t0, tE)\n",
    "        intervals.append(TierInterval(user_id, t0, c1, \"free\"))\n",
    "        intervals.append(TierInterval(user_id, c1, tE, \"premium\"))\n",
    "\n",
    "    elif cohort == 5:  # Free -> Premium -> Free\n",
    "        c1 = random_change_time(t0, tE - MIN_GAP_BETWEEN_TIER_CHANGES)\n",
    "        c2_low = c1 + MIN_GAP_BETWEEN_TIER_CHANGES\n",
    "        c2_high = tE\n",
    "        if c2_low + MIN_GAP_BETWEEN_TIER_CHANGES >= c2_high:\n",
    "            c2 = c1 + (tE - c1) / 2\n",
    "        else:\n",
    "            c2 = random_change_time(c2_low, c2_high)\n",
    "        intervals.append(TierInterval(user_id, t0, c1, \"free\"))\n",
    "        intervals.append(TierInterval(user_id, c1, c2, \"premium\"))\n",
    "        intervals.append(TierInterval(user_id, c2, tE, \"free\"))\n",
    "\n",
    "    intervals.sort(key=lambda x: x.from_dt)\n",
    "    fixed: list[TierInterval] = []\n",
    "    prev_end = None\n",
    "    for it in intervals:\n",
    "        if prev_end and it.from_dt < prev_end:\n",
    "            it = TierInterval(it.user_id, prev_end, it.until_dt, it.tier)\n",
    "        prev_end = it.until_dt\n",
    "        fixed.append(it)\n",
    "    return fixed\n",
    "\n",
    "def simulate_uploads_for_interval(user_id: int,\n",
    "                                  interval: TierInterval,\n",
    "                                  free_rate_per_hour: float,\n",
    "                                  prem_rate_per_hour: float,\n",
    "                                  next_txn_id: int) -> tuple[list[dict], int]:\n",
    "    \"\"\"\n",
    "    Simulate uploads for a single interval.\n",
    "    Returns uploads_rows, next_txn_id\n",
    "    \"\"\"\n",
    "    uploads_rows: list[dict] = []\n",
    "    tier = interval.tier\n",
    "    rate_per_hour = free_rate_per_hour if tier == \"free\" else prem_rate_per_hour\n",
    "\n",
    "    # Hourly grid across [from, until)\n",
    "    cursor = interval.from_dt\n",
    "    while cursor < interval.until_dt:\n",
    "        hour_end = min(cursor + timedelta(hours=1), interval.until_dt)\n",
    "        lam = rate_per_hour * (hour_end - cursor).total_seconds() / 3600.0\n",
    "        n = np.random.poisson(lam=lam) if lam > 0 else 0\n",
    "        for _ in range(n):\n",
    "            ts = bounded_random_time(cursor, hour_end)\n",
    "            if tier == \"free\":\n",
    "                item_id = 1  # 5 GB only\n",
    "            else:\n",
    "                # Premium outside spike window: choose with base probability\n",
    "                item_id = 2 if random.random() < BASE_P50_WHILE_PREMIUM else 1\n",
    "            uploads_rows.append({\n",
    "                \"txn_id\": next_txn_id,\n",
    "                \"user_id\": user_id,\n",
    "                \"item_id\": item_id,\n",
    "                \"datetime\": iso(ts)\n",
    "            })\n",
    "            next_txn_id += 1\n",
    "        cursor = hour_end\n",
    "\n",
    "    # Spike: within first hour after becoming premium\n",
    "    if tier == \"premium\":\n",
    "        became_premium_now = interval.from_dt > START\n",
    "        if became_premium_now and random.random() < P_SPIKE_50GB_WITHIN_1H:\n",
    "            spike_window_end = min(interval.from_dt + timedelta(hours=1), interval.until_dt)\n",
    "            if spike_window_end > interval.from_dt:\n",
    "                n_spike = 1 + (1 if (MAX_SPIKE_50GB > 1 and random.random() < 0.15) else 0)\n",
    "                for _ in range(n_spike):\n",
    "                    ts = bounded_random_time(interval.from_dt, spike_window_end)\n",
    "                    uploads_rows.append({\n",
    "                        \"txn_id\": next_txn_id,\n",
    "                        \"user_id\": user_id,\n",
    "                        \"item_id\": 2,  # force 50 GB in spike\n",
    "                        \"datetime\": iso(ts)\n",
    "                    })\n",
    "                    next_txn_id += 1\n",
    "\n",
    "    return uploads_rows, next_txn_id\n",
    "\n",
    "def main():\n",
    "    # Users\n",
    "    users = [{\"user_id\": uid, \"name\": f\"User {uid:03d}\"} for uid in range(1, N_USERS + 1)]\n",
    "\n",
    "    # Cohorts by user_id blocks of 20\n",
    "    def cohort_of(uid: int) -> int:\n",
    "        return (uid - 1) // 20 + 1  # 1..5\n",
    "\n",
    "    # User-specific rates\n",
    "    free_rate_per_hour = {u[\"user_id\"]: draw_user_rate(FREE_RATE_DAY_MEAN, FREE_RATE_DAY_STD) for u in users}\n",
    "    prem_rate_per_hour = {u[\"user_id\"]: draw_user_rate(PREM_RATE_DAY_MEAN, PREM_RATE_DAY_STD) for u in users}\n",
    "\n",
    "    # Tier intervals\n",
    "    tiers: list[TierInterval] = []\n",
    "    for u in users:\n",
    "        tiers.extend(generate_cohort_tiers(u[\"user_id\"], cohort_of(u[\"user_id\"])))\n",
    "    tiers.sort(key=lambda t: (t.user_id, t.from_dt))\n",
    "\n",
    "    # Simulate uploads\n",
    "    uploads_rows: list[dict] = []\n",
    "    next_txn_id = 1\n",
    "    for t in tiers:\n",
    "        ups, next_txn_id = simulate_uploads_for_interval(\n",
    "            user_id=t.user_id,\n",
    "            interval=t,\n",
    "            free_rate_per_hour=free_rate_per_hour[t.user_id],\n",
    "            prem_rate_per_hour=prem_rate_per_hour[t.user_id],\n",
    "            next_txn_id=next_txn_id\n",
    "        )\n",
    "        uploads_rows.extend(ups)\n",
    "\n",
    "    # Sort uploads by time (optional neatness)\n",
    "    uploads_rows.sort(key=lambda r: (r[\"user_id\"], r[\"datetime\"]))\n",
    "    # Re-assign txn_ids sequentially\n",
    "    for i, r in enumerate(uploads_rows, start=1):\n",
    "        r[\"txn_id\"] = i\n",
    "\n",
    "    # Write CSVs\n",
    "    with open(USERS_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"user_id\", \"name\"])\n",
    "        w.writeheader()\n",
    "        w.writerows(users)\n",
    "\n",
    "    with open(TIERS_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"user_id\", \"from_datetime\", \"until_datetime\", \"tier\"])\n",
    "        w.writeheader()\n",
    "        for t in tiers:\n",
    "            w.writerow({\n",
    "                \"user_id\": t.user_id,\n",
    "                \"from_datetime\": iso(t.from_dt),\n",
    "                \"until_datetime\": iso(t.until_dt),\n",
    "                \"tier\": t.tier\n",
    "            })\n",
    "\n",
    "    with open(ITEMS_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"item_id\", \"size_gb\"])\n",
    "        w.writeheader()\n",
    "        w.writerows(ITEMS)\n",
    "\n",
    "    with open(UPLOADS_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"txn_id\", \"user_id\", \"item_id\", \"datetime\"])\n",
    "        w.writeheader()\n",
    "        w.writerows(uploads_rows)\n",
    "\n",
    "    # Integrity check: no 50GB while free\n",
    "    by_user = {}\n",
    "    for t in tiers:\n",
    "        by_user.setdefault(t.user_id, []).append(t)\n",
    "\n",
    "    def tier_at(user_id: int, dt_str: str) -> str:\n",
    "        dt = datetime.strptime(dt_str, \"%Y-%m-%dT%H:%M:%SZ\").replace(tzinfo=timezone.utc)\n",
    "        for it in by_user[user_id]:\n",
    "            if it.from_dt <= dt < it.until_dt:\n",
    "                return it.tier\n",
    "        return \"unknown\"\n",
    "\n",
    "    violations = []\n",
    "    for up in uploads_rows:\n",
    "        t = tier_at(up[\"user_id\"], up[\"datetime\"])\n",
    "        size = ITEM_SIZE_BY_ID[up[\"item_id\"]]\n",
    "        if t == \"free\" and size == 50:\n",
    "            violations.append(up)\n",
    "\n",
    "    print(f\"Generated: {USERS_CSV}, {TIERS_CSV}, {ITEMS_CSV}, {UPLOADS_CSV}\")\n",
    "    if violations:\n",
    "        print(f\"[WARNING] Found {len(violations)} violations (50GB while free).\")\n",
    "    else:\n",
    "        print(\"Integrity check passed: No 50GB uploads while free.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2531fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 5: [('2025-03-01T00:00:00Z', '2025-03-11T00:00:00Z', 'premium')]\n",
      "User 25: [('2025-03-01T00:00:00Z', '2025-03-11T00:00:00Z', 'free')]\n",
      "User 45: [('2025-03-01T00:00:00Z', '2025-03-07T21:24:08Z', 'premium'), ('2025-03-07T21:24:08Z', '2025-03-11T00:00:00Z', 'free')]\n",
      "User 85: [('2025-03-01T00:00:00Z', '2025-03-06T10:47:41Z', 'free'), ('2025-03-06T10:47:41Z', '2025-03-09T00:22:06Z', 'premium'), ('2025-03-09T00:22:06Z', '2025-03-11T00:00:00Z', 'free')]\n"
     ]
    }
   ],
   "source": [
    "def get_user_tier_summary(user_id: int, tiers_csv: str = \"tiers.csv\"):\n",
    "    \"\"\"\n",
    "    Return a list of (start_iso, end_iso, tier) covering the user’s tier evolution.\n",
    "    - start/end are ISO-8601 UTC strings: 'YYYY-MM-DDTHH:MM:SSZ'\n",
    "    - tier is 'free' or 'premium'\n",
    "    \"\"\"\n",
    "    # Load and filter\n",
    "    tiers = pd.read_csv(tiers_csv)\n",
    "    tiers = tiers[tiers[\"user_id\"] == user_id].copy()\n",
    "    if tiers.empty:\n",
    "        return []  # unknown user_id\n",
    "\n",
    "    # Parse/normalize\n",
    "    tiers[\"from_datetime\"] = pd.to_datetime(tiers[\"from_datetime\"], utc=True)\n",
    "    tiers[\"until_datetime\"] = pd.to_datetime(tiers[\"until_datetime\"], utc=True)\n",
    "    tiers = tiers.sort_values([\"from_datetime\", \"until_datetime\", \"tier\"])\n",
    "\n",
    "    # Optional: coalesce adjacent intervals with the same tier\n",
    "    merged = []\n",
    "    for _, row in tiers.iterrows():\n",
    "        start, end, tier = row[\"from_datetime\"], row[\"until_datetime\"], row[\"tier\"]\n",
    "        if not merged:\n",
    "            merged.append([start, end, tier])\n",
    "        else:\n",
    "            last_start, last_end, last_tier = merged[-1]\n",
    "            # If same tier and touching (no gap), merge\n",
    "            if tier == last_tier and start == last_end:\n",
    "                merged[-1][1] = end\n",
    "            else:\n",
    "                merged.append([start, end, tier])\n",
    "\n",
    "    # Format to tuples with ISO-8601 Zulu\n",
    "    out = [(s.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "            e.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "            t) for s, e, t in merged]\n",
    "\n",
    "    return out\n",
    "\n",
    "# --- examples ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Always-Free or Always-Premium users will yield one tuple\n",
    "    print(\"User 5:\", get_user_tier_summary(5))\n",
    "    # Mixed cohorts will yield 2 or 3 tuples\n",
    "    print(\"User 25:\", get_user_tier_summary(25))\n",
    "    print(\"User 45:\", get_user_tier_summary(45))\n",
    "    print(\"User 85:\", get_user_tier_summary(85))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1784f85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
